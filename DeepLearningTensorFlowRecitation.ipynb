{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with TensorFlow\n",
    "### Recitation Notebook\n",
    "### Authors: Trevin Gandhi, Jordan Hurwitz, Brady Neal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recitation will consist of two parts:  \n",
    "[1) Building a feedforward Deep Neural Network in TensorFlow and discussing some best practices](#section1)  \n",
    "[2) Using TensorBoard for visualizations](#section2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#section1'></a>\n",
    "### Section 1: Building a Deep Feedforward Neural Network\n",
    "(Based on the TensorFlow tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick first thing to note --- for most applications of deep learning \n",
    "(for example, image recognition), instead of training a deep neural\n",
    "network from scratch (which can take on the order of days or weeks), it\n",
    "is common to download weights for pre-trained networks and \"fine-tune\"\n",
    "the network to fit your application. This allows you to train a neural \n",
    "network even when you don't have a bunch of data. However, the data \n",
    "that the pretrained model was trained on has to be similar \n",
    "to your data. \n",
    "\n",
    "In this notebook, however, we train the networks from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, we do the basic setup.\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph() # Just in case we're rerunning code in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# We will be training this deep neural network on MNIST,\n",
    "# so let's first load the dataset.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's initialize some placeholders\n",
    "\n",
    "# Here, x is a placeholder for our input data. Since MNIST\n",
    "# uses 28x28 pixel images, we \"unroll\" them into a 784-pixel\n",
    "# long vector. The `None` indicates that we can input an\n",
    "# arbitrary amount of datapoints. Thus we are saying x is a\n",
    "# matrix with 784 columns and an arbitrary (to be decided \n",
    "# when we supply the data) number of rows.\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# We define y to be the placeholder for our *true* y's. \n",
    "# We are giving y 10 rows because each row will be a\n",
    "# one-hot vector with the correct classification of the\n",
    "# image.\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "# y = tf.placeholder(tf.int32, shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we make a handy function for initializing biases. \n",
    "# Note that we are returning a \"Variable\" - this means\n",
    "# something that is subject to change during training.\n",
    "# TensorFlow is actually using gradient descent to optimize\n",
    "# the value of all \"Variables\" in our network. \n",
    "def bias_variable(shape):\n",
    "    # Here we just choose to initialize our biases to 0.\n",
    "    # However, this is not an agreed-upon standard and\n",
    "    # many just initialize the biases to 0.01 to ensure\n",
    "    # that all ReLU units fire in the beginning.\n",
    "    initial = tf.constant(0.00, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's define the first set of weights and biases (corresponding to our first layer)\n",
    "# We use He initialization for the weights as good practice for when we're training\n",
    "# deeper networks. Here, get_variable is similar to when we return a Variable and assign\n",
    "# it, except it also checks to see if the variable already exists.\n",
    "\n",
    "# This is: [number of input neurons, number of neurons in the first hidden layer,\n",
    "# number of neurons in the second hidden layer, number of classes]\n",
    "num_neurons = [784, 1280, 768, 10]\n",
    "\n",
    "# Just store this for convenience\n",
    "he_init  = tf.contrib.layers.variance_scaling_initializer()\n",
    "activ_fn = tf.nn.relu \n",
    "\n",
    "w1 = tf.get_variable(\"w1\", shape=[num_neurons[0], num_neurons[1]], \n",
    "                     initializer=he_init)\n",
    "b1 = bias_variable([num_neurons[1]])\n",
    "\n",
    "# Now let's define the computation that takes this layer's input and runs it through\n",
    "# the neurons. Note that we use the ReLU activation function to avoid problems\n",
    "# with our gradients. This line is the equivalent of saying the output of the\n",
    "# first hidden layer is max(x*w1 + b1, 0).\n",
    "h1 = activ_fn(tf.matmul(x, w1) + b1)\n",
    "\n",
    "# We also apply dropout after this layer and the next. Dropout is a form of regularization\n",
    "# in neural networks where we \"turn off\" randomly selected neurons during training.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h1_drop = tf.nn.dropout(h1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the second layer, similarly to the first.\n",
    "w2 = tf.get_variable(\"w2\", shape=[num_neurons[1], num_neurons[2]], \n",
    "                     initializer=he_init)\n",
    "b2 = bias_variable([num_neurons[2]])\n",
    "h2 = activ_fn(tf.matmul(h1_drop, w2) + b2)\n",
    "h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "\n",
    "# And define the third layer to output the log probabilities.\n",
    "# Note that this wouldn't really be considered a \"deep\" network\n",
    "# since there's only two hidden layers, but it should be clear to\n",
    "# see how hidden layers can easily be added at this point to make\n",
    "# it \"deep\".\n",
    "w3 = tf.get_variable(\"w3\", shape=[num_neurons[2], num_neurons[3]], \n",
    "                     initializer=he_init)\n",
    "b3 = bias_variable([num_neurons[3]])\n",
    "logits = tf.matmul(h2_drop, w3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We define our loss function to be cross entropy over softmax probabilities.\n",
    "# Here our true labels are defined by y, and our log probabilities\n",
    "# (TensorFlow calls them `logits`) are defined by logits.\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "# If we wanted, we could also add L2 weight regularization by adding\n",
    "# the following lines to the loss function\n",
    "#     0.0001*tf.nn.l2_loss(w1) +\\\n",
    "#     0.0001*tf.nn.l2_loss(w2) +\\\n",
    "#     0.0001*tf.nn.l2_loss(w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 completed: training accuracy 0.98, loss 0.0651423\n",
      "epoch 1 completed: training accuracy 0.98, loss 0.0514516\n",
      "epoch 2 completed: training accuracy 0.98, loss 0.0902712\n",
      "epoch 3 completed: training accuracy 0.98, loss 0.107601\n",
      "epoch 4 completed: training accuracy 0.96, loss 0.0628022\n",
      "epoch 5 completed: training accuracy 1, loss 0.0179585\n",
      "epoch 6 completed: training accuracy 0.98, loss 0.025748\n",
      "epoch 7 completed: training accuracy 0.96, loss 0.0735935\n",
      "epoch 8 completed: training accuracy 0.98, loss 0.0756909\n",
      "epoch 9 completed: training accuracy 1, loss 0.027951\n",
      "epoch 10 completed: training accuracy 0.98, loss 0.0581861\n",
      "epoch 11 completed: training accuracy 1, loss 0.0085786\n",
      "epoch 12 completed: training accuracy 1, loss 0.00981028\n",
      "epoch 13 completed: training accuracy 1, loss 0.00588983\n",
      "epoch 14 completed: training accuracy 1, loss 0.0120707\n",
      "epoch 15 completed: training accuracy 0.98, loss 0.0669131\n",
      "epoch 16 completed: training accuracy 1, loss 0.0121754\n",
      "epoch 17 completed: training accuracy 1, loss 0.00125657\n",
      "epoch 18 completed: training accuracy 1, loss 0.00705501\n",
      "epoch 19 completed: training accuracy 1, loss 0.000712952\n",
      "test accuracy 0.9831\n"
     ]
    }
   ],
   "source": [
    "# We will use the `Adam` optimizer. Adam is an fancier variant of\n",
    "# standard gradient descent.\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Here we build a binary vector corresponding to where our predicted \n",
    "# classes matched the actual classes.\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    num_train = mnist.train.num_examples\n",
    "    num_test  = mnist.test.num_examples\n",
    "\n",
    "    num_epochs = 20\n",
    "    batch_size = 50\n",
    "    \n",
    "    # Train\n",
    "    for i in range(num_epochs):\n",
    "        for _ in range(num_train / batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "        # Get an estimate of our current progress using the last batch\n",
    "        train_accuracy, loss = sess.run([accuracy, cross_entropy], \n",
    "                                    feed_dict={x:batch[0], y: batch[1], keep_prob: 1.0})\n",
    "        print(\"epoch %d completed: training accuracy %g, loss %g\"%(i, train_accuracy, loss))\n",
    "\n",
    "    # Test\n",
    "    test_accuracy = 0\n",
    "    for _ in range(num_test / batch_size):\n",
    "        batch = mnist.test.next_batch(batch_size)\n",
    "        test_accuracy += batch_size * accuracy.eval(feed_dict={\n",
    "            x:batch[0], y: batch[1], keep_prob: 1.0})\n",
    "\n",
    "    print(\"test accuracy %g\"%(test_accuracy / num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we make this simpler? \n",
    "With TensorFlow 1.0, we can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected, dropout, batch_norm\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x  = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "num_neurons = [784, 1280, 768, 10]\n",
    "he_init  = tf.contrib.layers.variance_scaling_initializer()\n",
    "activ_fn = tf.nn.relu \n",
    "\n",
    "# Instead of making keep_prob a placeholder (like we did for dropout\n",
    "# above), we can make a boolean `is_training` placeholder that dropout\n",
    "# and batch normalization can check to determine what parameter\n",
    "# values to use (i.e. if is_training = True, then dropout will use\n",
    "# a keep_prob of 0.5. Otherwise, it uses a keep_prob of 1.0).\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "# We can even easily add Batch Normalization, which can also be quite\n",
    "# useful when training deep neural networks (although it won't do much\n",
    "# here).\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None\n",
    "}\n",
    "\n",
    "# Define the first hidden layer using `fully_connected`\n",
    "# There are similar functions (e.g. conv2d) for other\n",
    "# types of layers.\n",
    "keep_prob = 0.5\n",
    "hidden1 = fully_connected(x, num_neurons[1], \n",
    "                          weights_initializer=he_init,\n",
    "                          activation_fn=activ_fn,\n",
    "                          normalizer_fn=batch_norm, \n",
    "                          normalizer_params=bn_params)\n",
    "hidden1_drop = dropout(hidden1, keep_prob, is_training=is_training)\n",
    "\n",
    "hidden2 = fully_connected(hidden1_drop, num_neurons[2], \n",
    "                          weights_initializer=he_init,\n",
    "                          activation_fn=activ_fn,\n",
    "                          normalizer_fn=batch_norm, \n",
    "                          normalizer_params=bn_params)\n",
    "hidden2_drop = dropout(hidden2, keep_prob, is_training=is_training)\n",
    "\n",
    "logits = fully_connected(hidden2_drop, num_neurons[3], activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 completed: training accuracy 0.9, loss 0.273912\n",
      "epoch 1 completed: training accuracy 1, loss 0.0604001\n",
      "epoch 2 completed: training accuracy 0.98, loss 0.17269\n",
      "epoch 3 completed: training accuracy 0.92, loss 0.217366\n",
      "epoch 4 completed: training accuracy 0.98, loss 0.121496\n",
      "epoch 5 completed: training accuracy 1, loss 0.00895906\n",
      "epoch 6 completed: training accuracy 0.98, loss 0.0607898\n",
      "epoch 7 completed: training accuracy 1, loss 0.0279312\n",
      "epoch 8 completed: training accuracy 1, loss 0.00784993\n",
      "epoch 9 completed: training accuracy 1, loss 0.0268133\n",
      "epoch 10 completed: training accuracy 1, loss 0.0107311\n",
      "epoch 11 completed: training accuracy 0.98, loss 0.155614\n",
      "epoch 12 completed: training accuracy 1, loss 0.00265834\n",
      "epoch 13 completed: training accuracy 1, loss 0.00531178\n",
      "epoch 14 completed: training accuracy 1, loss 0.00908255\n",
      "epoch 15 completed: training accuracy 1, loss 0.00213866\n",
      "epoch 16 completed: training accuracy 0.98, loss 0.0469729\n",
      "epoch 17 completed: training accuracy 1, loss 0.00695472\n",
      "epoch 18 completed: training accuracy 1, loss 0.00482372\n",
      "epoch 19 completed: training accuracy 1, loss 0.00653204\n",
      "test accuracy 0.9838\n"
     ]
    }
   ],
   "source": [
    "# Let's train it and see how it does! It should be pretty similar\n",
    "# to our previous results.\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    num_train = mnist.train.num_examples\n",
    "    num_test  = mnist.test.num_examples\n",
    "\n",
    "    num_epochs = 20\n",
    "    batch_size = 50\n",
    "    \n",
    "    # Train\n",
    "    for i in range(num_epochs):\n",
    "        for _ in range(num_train / batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1], is_training: True})\n",
    "        # Get an estimate of our current progress using the last batch\n",
    "        train_accuracy, loss = sess.run([accuracy, cross_entropy], \n",
    "                                    feed_dict={x:batch[0], y: batch[1], is_training: False})\n",
    "        print(\"epoch %d completed: training accuracy %g, loss %g\"%(i, train_accuracy, loss))\n",
    "\n",
    "    # Test\n",
    "    test_accuracy = 0\n",
    "    for _ in range(num_test / batch_size):\n",
    "        batch = mnist.test.next_batch(batch_size)\n",
    "        test_accuracy += batch_size * accuracy.eval(feed_dict={\n",
    "            x:batch[0], y: batch[1], is_training: False})\n",
    "\n",
    "    print(\"test accuracy %g\"%(test_accuracy / num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#section2'></a>\n",
    "### Using TensorBoard for Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See DeepLearningTensorFlowRecitation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Classification with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we will train another classifier for the MNIST dataset\n",
    "# except this time, we will use an RNN. While this may not be\n",
    "# an especially intuitive application, in my opinion, it is an\n",
    "# interesting (although also not very practical) application of\n",
    "# RNNs for that reason. \n",
    "\n",
    "# So how do we do this? Since the images are 28 x 28 pixels, we\n",
    "# will model them as a sequence of 28 pixel vectors across 28 \n",
    "# timesteps. We will feed each of these pixel vectors into a\n",
    "# GRU cell with 150 neurons. At the end of the 28 timesteps, \n",
    "# we will take the state of the RNN and feed it into a fully\n",
    "# connected layer with 10 outputs, allowing us to generate\n",
    "# log probabilities for each of the classes. Then, the rest\n",
    "# proceeds as above where we can do softmax and cross-entropy\n",
    "# on the log probabilities to determine the loss, and use that\n",
    "# loss for backpropagation through the network.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "num_timesteps = 28\n",
    "# [num inputs per timestep, num neurons in RNN Cell, num outputs for fully connected layer]\n",
    "num_neurons = [28, 150, 10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since the input data initially comes as a 784-dimension vector,\n",
    "# we need to reshape it back into \n",
    "x = tf.placeholder(tf.float32, [None, num_timesteps, num_neurons[0]])\n",
    "# y = tf.placeholder(tf.int32, shape=[None])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Here is where we define the core of the network. Right now, \n",
    "# we are using a GRU cell with 150 neurons. We will then feed\n",
    "# it into the dynamic_rnn function, which will run all the \n",
    "# timesteps for the RNN. Note that since we know the number of \n",
    "# timesteps for every input, we could use the static_rnn function.\n",
    "# However, the dynamic_rnn function seems to be strictly better as\n",
    "# it has an easier API (don't need to stack and unstack the data)\n",
    "# and can even support offloading GPU memory to CPU memory to avoid\n",
    "# OutOfMemory errors.\n",
    "basic_cell = tf.contrib.rnn.GRUCell(num_units=num_neurons[1])\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, x, dtype=tf.float32)\n",
    "\n",
    "# Now we take the final state of the RNN and feed it into a\n",
    "# fully connected layer to obtain our log probabilities.\n",
    "logits = fully_connected(states, num_neurons[2], activation_fn=None)\n",
    "\n",
    "# From here on, this code should seem familiar as it is essentially\n",
    "# the same code as above.\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 completed: training accuracy 0.946667, loss 0.144265\n",
      "epoch 1 completed: training accuracy 0.966667, loss 0.0917355\n",
      "epoch 2 completed: training accuracy 0.946667, loss 0.185444\n",
      "epoch 3 completed: training accuracy 0.966667, loss 0.0695253\n",
      "epoch 4 completed: training accuracy 0.986667, loss 0.0532333\n",
      "epoch 5 completed: training accuracy 0.993333, loss 0.0343793\n",
      "epoch 6 completed: training accuracy 0.986667, loss 0.0229701\n",
      "epoch 7 completed: training accuracy 0.986667, loss 0.0214251\n",
      "epoch 8 completed: training accuracy 1, loss 0.0106468\n",
      "epoch 9 completed: training accuracy 1, loss 0.0209855\n",
      "epoch 10 completed: training accuracy 1, loss 0.00439281\n",
      "epoch 11 completed: training accuracy 1, loss 0.00868359\n",
      "epoch 12 completed: training accuracy 1, loss 0.00909726\n",
      "epoch 13 completed: training accuracy 0.993333, loss 0.0184517\n",
      "epoch 14 completed: training accuracy 1, loss 0.00132668\n",
      "epoch 15 completed: training accuracy 1, loss 0.00163033\n",
      "epoch 16 completed: training accuracy 0.993333, loss 0.0157659\n",
      "epoch 17 completed: training accuracy 1, loss 0.00497101\n",
      "epoch 18 completed: training accuracy 1, loss 0.00420386\n",
      "epoch 19 completed: training accuracy 0.993333, loss 0.00634408\n",
      "epoch 20 completed: training accuracy 1, loss 0.00228094\n",
      "epoch 21 completed: training accuracy 1, loss 0.0026408\n",
      "epoch 22 completed: training accuracy 0.993333, loss 0.0286598\n",
      "epoch 23 completed: training accuracy 1, loss 0.000505611\n",
      "epoch 24 completed: training accuracy 1, loss 0.00727361\n",
      "epoch 25 completed: training accuracy 1, loss 0.000566558\n",
      "epoch 26 completed: training accuracy 1, loss 0.000741867\n",
      "epoch 27 completed: training accuracy 1, loss 0.00374561\n",
      "epoch 28 completed: training accuracy 1, loss 0.00332795\n",
      "epoch 29 completed: training accuracy 1, loss 0.00057965\n",
      "epoch 30 completed: training accuracy 0.993333, loss 0.00878199\n",
      "epoch 31 completed: training accuracy 1, loss 0.00263774\n",
      "epoch 32 completed: training accuracy 1, loss 0.000208338\n",
      "epoch 33 completed: training accuracy 1, loss 0.000256371\n",
      "epoch 34 completed: training accuracy 1, loss 0.000192443\n",
      "epoch 35 completed: training accuracy 1, loss 0.00293898\n",
      "epoch 36 completed: training accuracy 1, loss 0.00203977\n",
      "epoch 37 completed: training accuracy 1, loss 0.000428889\n",
      "epoch 38 completed: training accuracy 1, loss 0.000293098\n",
      "epoch 39 completed: training accuracy 1, loss 0.00131604\n",
      "epoch 40 completed: training accuracy 1, loss 0.00213742\n",
      "epoch 41 completed: training accuracy 1, loss 0.000595155\n",
      "epoch 42 completed: training accuracy 1, loss 0.0053142\n",
      "epoch 43 completed: training accuracy 1, loss 0.00093572\n",
      "epoch 44 completed: training accuracy 1, loss 0.000404495\n",
      "epoch 45 completed: training accuracy 1, loss 0.000601842\n",
      "epoch 46 completed: training accuracy 1, loss 0.00028945\n",
      "epoch 47 completed: training accuracy 1, loss 0.000758605\n",
      "epoch 48 completed: training accuracy 1, loss 3.43045e-05\n",
      "epoch 49 completed: training accuracy 1, loss 0.00102514\n",
      "epoch 50 completed: training accuracy 1, loss 0.000506077\n",
      "epoch 51 completed: training accuracy 1, loss 0.000379454\n",
      "epoch 52 completed: training accuracy 0.993333, loss 0.00667775\n",
      "epoch 53 completed: training accuracy 1, loss 0.000211553\n",
      "epoch 54 completed: training accuracy 1, loss 0.000392445\n",
      "epoch 55 completed: training accuracy 1, loss 0.00304417\n",
      "epoch 56 completed: training accuracy 1, loss 0.000147135\n",
      "epoch 57 completed: training accuracy 1, loss 0.00199415\n",
      "epoch 58 completed: training accuracy 0.993333, loss 0.00683369\n",
      "epoch 59 completed: training accuracy 1, loss 0.000458594\n",
      "epoch 60 completed: training accuracy 1, loss 0.000271766\n",
      "epoch 61 completed: training accuracy 1, loss 0.000861117\n",
      "epoch 62 completed: training accuracy 1, loss 4.86524e-05\n",
      "epoch 63 completed: training accuracy 1, loss 0.00144079\n",
      "epoch 64 completed: training accuracy 1, loss 6.8515e-05\n",
      "epoch 65 completed: training accuracy 1, loss 0.00250631\n",
      "epoch 66 completed: training accuracy 1, loss 0.000581219\n",
      "epoch 67 completed: training accuracy 1, loss 0.000121927\n",
      "epoch 68 completed: training accuracy 1, loss 0.000228271\n",
      "epoch 69 completed: training accuracy 1, loss 0.00132923\n",
      "epoch 70 completed: training accuracy 1, loss 0.000291989\n",
      "epoch 71 completed: training accuracy 1, loss 0.00040644\n",
      "epoch 72 completed: training accuracy 1, loss 4.991e-05\n",
      "epoch 73 completed: training accuracy 1, loss 0.000375519\n",
      "epoch 74 completed: training accuracy 1, loss 0.00024048\n",
      "epoch 75 completed: training accuracy 1, loss 0.000177058\n",
      "epoch 76 completed: training accuracy 1, loss 0.000870598\n",
      "epoch 77 completed: training accuracy 1, loss 0.0017774\n",
      "epoch 78 completed: training accuracy 1, loss 4.57094e-05\n",
      "epoch 79 completed: training accuracy 1, loss 0.000334025\n",
      "epoch 80 completed: training accuracy 1, loss 7.29465e-06\n",
      "epoch 81 completed: training accuracy 1, loss 3.77897e-05\n",
      "epoch 82 completed: training accuracy 1, loss 1.02839e-05\n",
      "epoch 83 completed: training accuracy 1, loss 6.94818e-06\n",
      "epoch 84 completed: training accuracy 1, loss 1.73532e-05\n",
      "epoch 85 completed: training accuracy 1, loss 7.6933e-06\n",
      "epoch 86 completed: training accuracy 1, loss 9.90225e-07\n",
      "epoch 87 completed: training accuracy 1, loss 3.12314e-06\n",
      "epoch 88 completed: training accuracy 1, loss 1.38837e-06\n",
      "epoch 89 completed: training accuracy 1, loss 4.3785e-06\n",
      "epoch 90 completed: training accuracy 1, loss 1.12897e-05\n",
      "epoch 91 completed: training accuracy 1, loss 5.93874e-06\n",
      "epoch 92 completed: training accuracy 1, loss 2.75122e-06\n",
      "epoch 93 completed: training accuracy 1, loss 1.51233e-06\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    num_train = mnist.train.num_examples\n",
    "\n",
    "    num_epochs = 100\n",
    "    batch_size = 150\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Train\n",
    "    for i in range(num_epochs):\n",
    "        for _ in range(num_train // batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            x_batch = batch[0].reshape((-1, num_timesteps, num_neurons[0]))\n",
    "            sess.run(train_step, feed_dict={x: x_batch, y: batch[1]})\n",
    "        train_accuracy, loss = sess.run([accuracy, cross_entropy], \n",
    "                                        feed_dict={x: x_batch, y: batch[1]})\n",
    "        print(\"epoch %d completed: training accuracy %g, loss %g\"%(i, train_accuracy, loss))\n",
    "        \n",
    "    # Test\n",
    "    x_test = mnist.test.images.reshape((-1, num_timesteps, num_neurons[0]))\n",
    "    y_test = mnist.test.labels\n",
    "    test_accuracy = accuracy.eval(feed_dict={x: x_test, y: y_test})\n",
    "    print(\"test accuracy %g\"%(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation with RNNs\n",
    "Based off the TensorFlow Tutorial and Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us first download the dataset we will be using,\n",
    "# the works of Shakespeare. Dataset from Andrej Karpathy.\n",
    "import urllib2\n",
    "print ('Downloading Shakespeare data')\n",
    "source = urllib2.urlopen(\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
    "shakespeare = source.read()\n",
    "print ('Download complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lstm_size = 512\n",
    "# lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size, state_is_tuple=False)\n",
    "# stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm, lstm],\n",
    "#     state_is_tuple=False)\n",
    "# initial_state = state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "# for i in range(num_steps):\n",
    "#     # The value of state is updated after processing each batch of words.\n",
    "#     output, state = stacked_lstm(words[:, i], state)\n",
    "\n",
    "#     # The rest of the code.\n",
    "#     # ...\n",
    " \n",
    "# final_state = state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

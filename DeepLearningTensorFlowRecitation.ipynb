{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with TensorFlow\n",
    "## Recitation Notebook\n",
    "\n",
    "### Authors: Trevin Gandhi, Jordan Hurwitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recitation will consist of two parts:  \n",
    "1) Building a feedforward Deep Neural Network in TensorFlow and discussing some best practices  \n",
    "2) Building an RNN (specifically, an LSTM) in TensorFlow and using it for text generation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Building a Deep Feedforward Neural Network\n",
    "(Based on the TensorFlow tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, we do the basic setup.\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# We will be training this deep neural network on MNIST,\n",
    "# so let's first load the dataset.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's initialize some placeholders\n",
    "\n",
    "# Here, x is a placeholder for our input data. Since MNIST\n",
    "# uses 28x28 pixel images, we \"unroll\" them into a 784-pixel\n",
    "# long vector. The `None` indicates that we can input an\n",
    "# arbitrary amount of datapoints. Thus we are saying x is a\n",
    "# matrix with 784 columns and an arbitrary (to be decided \n",
    "# when we supply the data) number of rows.\n",
    "x  = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# We define y_ to be the placeholder for our *true* y's. \n",
    "# We are giving y_ 10 rows because each row will be a\n",
    "# one-hot vector with the correct classification of the\n",
    "# image.\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0527015 ,  0.16926853,  0.02503373, -0.04121994,  0.02684036,\n",
       "        -0.01603753, -0.06777612, -0.04115577, -0.04395923, -0.09951807],\n",
       "       [ 0.06317057, -0.06287027,  0.04967545, -0.16697605,  0.00188705,\n",
       "        -0.09903455,  0.12493267,  0.08164623, -0.05969295,  0.04415423],\n",
       "       [-0.0604263 ,  0.00805518,  0.09937415, -0.09267912,  0.05353724,\n",
       "        -0.08030748, -0.08336922,  0.0610524 , -0.0877239 ,  0.03455708],\n",
       "       [ 0.0082681 , -0.0897309 , -0.03808483, -0.04349993, -0.07961068,\n",
       "         0.00809762, -0.05464205, -0.12478473, -0.10297551,  0.03020481]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tf.truncated_normal([4, 10], stddev=0.1)\n",
    "sess.run(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we make a handy function for initializing biases. \n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's define the first set of weights and biases (corresponding to our first layer)\n",
    "# We use Xavier initialization for the weights as good practice for when we're training\n",
    "# deeper networks.\n",
    "num_neurons = 512\n",
    "w1 = tf.get_variable(\"w1\", shape=[784, num_neurons], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = bias_variable([num_neurons])\n",
    "\n",
    "# Now let's define the computation that takes this layer's input and runs it through\n",
    "# the neurons. Note that we use the ReLU activation function to avoid problems\n",
    "# with our gradients.\n",
    "h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "\n",
    "# We also apply dropout after this layer and the next. Dropout is a form of regularization\n",
    "# in neural networks where we \"turn off\" randomly selected neurons during training.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h1_drop = tf.nn.dropout(h1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the second layer, similarly to the first.\n",
    "w2 = tf.get_variable(\"w2\", shape=[num_neurons, num_neurons], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = bias_variable([num_neurons])\n",
    "h2 = tf.nn.relu(tf.matmul(h1_drop, w2) + b2)\n",
    "h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "\n",
    "# And define the third layer to output the log probabilities \n",
    "w3 = tf.get_variable(\"w3\", shape=[num_neurons, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = bias_variable([10])\n",
    "y  = tf.matmul(h2_drop, w3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define our loss function to be cross entropy over softmax probabilities.\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.18\n",
      "step 500, training accuracy 0.84\n",
      "step 1000, training accuracy 0.9\n",
      "step 1500, training accuracy 0.94\n",
      "step 2000, training accuracy 0.94\n",
      "step 2500, training accuracy 0.9\n",
      "step 3000, training accuracy 0.92\n",
      "step 3500, training accuracy 0.96\n",
      "step 4000, training accuracy 0.94\n",
      "step 4500, training accuracy 1\n",
      "step 5000, training accuracy 0.98\n",
      "step 5500, training accuracy 1\n",
      "step 6000, training accuracy 0.98\n",
      "step 6500, training accuracy 1\n",
      "step 7000, training accuracy 0.94\n",
      "step 7500, training accuracy 0.92\n",
      "step 8000, training accuracy 1\n",
      "step 8500, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9500, training accuracy 0.98\n",
      "step 10000, training accuracy 1\n",
      "step 10500, training accuracy 0.98\n",
      "step 11000, training accuracy 0.98\n",
      "step 11500, training accuracy 0.98\n",
      "step 12000, training accuracy 0.98\n",
      "step 12500, training accuracy 1\n",
      "step 13000, training accuracy 0.98\n",
      "step 13500, training accuracy 1\n",
      "step 14000, training accuracy 0.98\n",
      "step 14500, training accuracy 1\n",
      "step 15000, training accuracy 0.96\n",
      "step 15500, training accuracy 0.96\n",
      "step 16000, training accuracy 1\n",
      "step 16500, training accuracy 0.98\n",
      "step 17000, training accuracy 1\n",
      "step 17500, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18500, training accuracy 0.98\n",
      "step 19000, training accuracy 0.96\n",
      "step 19500, training accuracy 1\n",
      "test accuracy 0.9806\n"
     ]
    }
   ],
   "source": [
    "# We will use the `Adam` optimizer. Adam is an improved variant of\n",
    "# Stochastic Gradient Descent.\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%500 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "# Need to change this to be clean\n",
    "test_accuracy = 0\n",
    "for i in range(20):\n",
    "    batch = mnist.test.next_batch(500)\n",
    "    test_accuracy += 500 * accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "\n",
    "print(\"test accuracy %g\"%(test_accuracy / 10000))\n",
    "# print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "#     x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
